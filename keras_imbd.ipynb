{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data is a list of words in a review.\n",
    "\n",
    "It's not the actual words, but rather integers representing word ids\n",
    "\n",
    "```\n",
    "[1,\n",
    " 14,\n",
    " 22,\n",
    " 16,\n",
    " 43,\n",
    " ...\n",
    " ]\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_word_index()` is a mapping from text --> id (int)\n",
    "\n",
    "```\n",
    "{\n",
    "    \"harpo's\": 40831,\n",
    "    'canet': 52011,\n",
    "    'aileen': 19313,\n",
    "    'acurately': 52012,\n",
    "    \"diplomat's\": 52013,\n",
    "    'rickman': 25242,\n",
    "    'arranged': 6746,\n",
    "    'rumbustious': 52014,\n",
    "    'familiarness': 52015,\n",
    "    \"spider'\": 52016,\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "The mapping can then be turned back into words:\n",
    "\n",
    "\n",
    "```\n",
    "['?',\n",
    " 'this',\n",
    " 'film',\n",
    " 'was',\n",
    " 'just',\n",
    " 'brilliant',\n",
    " 'casting',\n",
    " 'location',\n",
    " 'scenery',\n",
    " 'story',\n",
    " 'direction',\n",
    " \"everyone's\",\n",
    " 'really',\n",
    " ...\n",
    " ]\n",
    "\n",
    "```\n",
    "\n",
    "Strangely we have to offset by 3. Perhaps it's explained in the dataset but 0,1,2 are reserved words for start, end, etc. \n",
    "\n",
    "Note that this actually reads like a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word = {idx: word for word, idx in imdb.get_word_index().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['?',\n",
       " 'this',\n",
       " 'film',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'casting',\n",
       " 'location',\n",
       " 'scenery',\n",
       " 'story',\n",
       " 'direction',\n",
       " \"everyone's\",\n",
       " 'really',\n",
       " 'suited',\n",
       " 'the',\n",
       " 'part',\n",
       " 'they',\n",
       " 'played',\n",
       " 'and',\n",
       " 'you',\n",
       " 'could',\n",
       " 'just',\n",
       " 'imagine',\n",
       " 'being',\n",
       " 'there',\n",
       " 'robert',\n",
       " '?',\n",
       " 'is',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'actor',\n",
       " 'and',\n",
       " 'now',\n",
       " 'the',\n",
       " 'same',\n",
       " 'being',\n",
       " 'director',\n",
       " '?',\n",
       " 'father',\n",
       " 'came',\n",
       " 'from',\n",
       " 'the',\n",
       " 'same',\n",
       " 'scottish',\n",
       " 'island',\n",
       " 'as',\n",
       " 'myself',\n",
       " 'so',\n",
       " 'i',\n",
       " 'loved',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'real',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'this',\n",
       " 'film',\n",
       " 'the',\n",
       " 'witty',\n",
       " 'remarks',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'film',\n",
       " 'were',\n",
       " 'great',\n",
       " 'it',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'so',\n",
       " 'much',\n",
       " 'that',\n",
       " 'i',\n",
       " 'bought',\n",
       " 'the',\n",
       " 'film',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'it',\n",
       " 'was',\n",
       " 'released',\n",
       " 'for',\n",
       " '?',\n",
       " 'and',\n",
       " 'would',\n",
       " 'recommend',\n",
       " 'it',\n",
       " 'to',\n",
       " 'everyone',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fly',\n",
       " 'fishing',\n",
       " 'was',\n",
       " 'amazing',\n",
       " 'really',\n",
       " 'cried',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'it',\n",
       " 'was',\n",
       " 'so',\n",
       " 'sad',\n",
       " 'and',\n",
       " 'you',\n",
       " 'know',\n",
       " 'what',\n",
       " 'they',\n",
       " 'say',\n",
       " 'if',\n",
       " 'you',\n",
       " 'cry',\n",
       " 'at',\n",
       " 'a',\n",
       " 'film',\n",
       " 'it',\n",
       " 'must',\n",
       " 'have',\n",
       " 'been',\n",
       " 'good',\n",
       " 'and',\n",
       " 'this',\n",
       " 'definitely',\n",
       " 'was',\n",
       " 'also',\n",
       " '?',\n",
       " 'to',\n",
       " 'the',\n",
       " 'two',\n",
       " 'little',\n",
       " \"boy's\",\n",
       " 'that',\n",
       " 'played',\n",
       " 'the',\n",
       " '?',\n",
       " 'of',\n",
       " 'norman',\n",
       " 'and',\n",
       " 'paul',\n",
       " 'they',\n",
       " 'were',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'children',\n",
       " 'are',\n",
       " 'often',\n",
       " 'left',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " '?',\n",
       " 'list',\n",
       " 'i',\n",
       " 'think',\n",
       " 'because',\n",
       " 'the',\n",
       " 'stars',\n",
       " 'that',\n",
       " 'play',\n",
       " 'them',\n",
       " 'all',\n",
       " 'grown',\n",
       " 'up',\n",
       " 'are',\n",
       " 'such',\n",
       " 'a',\n",
       " 'big',\n",
       " 'profile',\n",
       " 'for',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'film',\n",
       " 'but',\n",
       " 'these',\n",
       " 'children',\n",
       " 'are',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'should',\n",
       " 'be',\n",
       " 'praised',\n",
       " 'for',\n",
       " 'what',\n",
       " 'they',\n",
       " 'have',\n",
       " 'done',\n",
       " \"don't\",\n",
       " 'you',\n",
       " 'think',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'story',\n",
       " 'was',\n",
       " 'so',\n",
       " 'lovely',\n",
       " 'because',\n",
       " 'it',\n",
       " 'was',\n",
       " 'true',\n",
       " 'and',\n",
       " 'was',\n",
       " \"someone's\",\n",
       " 'life',\n",
       " 'after',\n",
       " 'all',\n",
       " 'that',\n",
       " 'was',\n",
       " 'shared',\n",
       " 'with',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx_to_word.get(idx-3, '?') for idx in train_data[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences have different lengths, as we might expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_lengths = [len(sent) for sent in train_data]\n",
    "sent_lengths = pd.Series(sent_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    218\n",
       "1    189\n",
       "2    141\n",
       "3    550\n",
       "4    147\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_lengths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Someone went through and marked all the labels, positive or negative\n",
    "\n",
    "This is much simpler because each review is either positive or neg, so there isn't much procssing to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_sents(sequences, dimensions=10000):\n",
    "    matrix = np.zeros((len(sequences), dimensions))\n",
    "    for i, seq in enumerate(sequences):\n",
    "        matrix[i, seq] = 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sents(train_data)\n",
    "x_test = vectorize_sents(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels, dtype=np.float32)\n",
    "y_test = np.asarray(test_labels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each step is a dot product\n",
    "\n",
    "```\n",
    "    output = relu(dot(W, input) + b)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the params calculated for each layer?\n",
    "\n",
    "1) `10000 * 16 + 16`\n",
    "\n",
    "10,000 is the input, 16 is the number of nodes in the hidden layer, and one bias term for each hidden layer (16)\n",
    "\n",
    "2) `16 * 16 + 16`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                160016    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,305\n",
      "Trainable params: 160,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16 * 16 + 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: instead of using the string names for the parameters, there are configurable objects.\n",
    "\n",
    "e.g.\n",
    "\n",
    "** Metrics **\n",
    "```\n",
    "metrics.K\n",
    "metrics.MAE\n",
    "metrics.MAPE\n",
    "metrics.MSE\n",
    "metrics.absolute_import\n",
    "metrics.binary_accuracy\n",
    "metrics.binary_crossentropy\n",
    "metrics.categorical_accuracy\n",
    "metrics.categorical_crossentropy\n",
    "metrics.cosine\n",
    "metrics.cosine_proximity\n",
    "metrics.deserialize\n",
    "metrics.deserialize_keras_object\n",
    "metrics.get\n",
    "metrics.hinge\n",
    "metrics.kullback_leibler_divergence\n",
    "metrics.logcosh\n",
    "metrics.mae\n",
    "metrics.mape\n",
    "metrics.mean_absolute_error\n",
    "metrics.mean_absolute_percentage_error\n",
    "metrics.mean_squared_error\n",
    "metrics.mean_squared_logarithmic_error\n",
    "metrics.mse\n",
    "metrics.msle\n",
    "metrics.poisson\n",
    "metrics.serialize\n",
    "metrics.six\n",
    "metrics.sparse_categorical_accuracy\n",
    "metrics.sparse_categorical_crossentropy\n",
    "metrics.squared_hinge\n",
    "metrics.top_k_categorical_accuracy\n",
    "\n",
    "```\n",
    "** Optimizers **\n",
    "\n",
    "```\n",
    "optimizers.Adadelta\n",
    "optimizers.Adagrad\n",
    "optimizers.Adam\n",
    "optimizers.Adamax\n",
    "optimizers.K\n",
    "optimizers.Nadam\n",
    "optimizers.Optimizer\n",
    "optimizers.RMSprop\n",
    "optimizers.SGD\n",
    "optimizers.TFOptimizer\n",
    "optimizers.absolute_import\n",
    "optimizers.adadelta\n",
    "optimizers.adagrad\n",
    "optimizers.adam\n",
    "optimizers.adamax\n",
    "optimizers.clip_norm\n",
    "optimizers.nadam\n",
    "optimizers.rmsprop\n",
    "optimizers.sgd\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizers.*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a validation set\n",
    "\n",
    "\n",
    "From our training data, section it off into training and validation\n",
    "\n",
    "This doesn't just seem like a formality, it's a necessity because we need to pass validation data into our model to see how it compares against 'unknown' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 10000), (25000, 10000))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partial_x_train = x_train[:10000]\n",
    "partial_y_train = y_train[:10000]\n",
    "\n",
    "\n",
    "x_val = x_train[10000:]\n",
    "y_val = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 0.4187 - acc: 0.8568 - val_loss: 0.4175 - val_acc: 0.8350\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train, \n",
    "                    partial_y_train,\n",
    "                   epochs=1,\n",
    "                   batch_size=512,\n",
    "                   validation_data=(x_val[:1000], y_val[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.98699999380111691],\n",
       " 'loss': [0.30169386696815492],\n",
       " 'val_acc': [0.78999998569488528],\n",
       " 'val_loss': [0.5083364133834839]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
